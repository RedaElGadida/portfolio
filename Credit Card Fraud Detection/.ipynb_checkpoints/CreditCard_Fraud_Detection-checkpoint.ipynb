{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-markdown-1",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection: A Machine Learning Approach\n",
    "\n",
    "**Project Goal:** To build and evaluate a machine learning model capable of accurately identifying fraudulent credit card transactions from a highly imbalanced dataset.\n",
    "\n",
    "**The Challenge:** In real-world financial data, fraudulent transactions are extremely rare compared to legitimate ones. This creates a significant class imbalance, which can lead a naive model to achieve high accuracy by simply predicting every transaction as legitimate, thereby failing to catch any fraud. This project focuses on strategies to overcome this challenge.\n",
    "\n",
    "**Methodology:**\n",
    "1.  **Exploratory Data Analysis (EDA):** Understand the data's structure, features, and the extent of the class imbalance.\n",
    "2.  **Data Preprocessing:** Scale the features to ensure the model treats all inputs fairly.\n",
    "3.  **Handling Imbalance:** Experiment with two key techniques: Random Undersampling and SMOTE (Synthetic Minority Over-sampling Technique).\n",
    "4.  **Modeling & Evaluation:** Train multiple models (Logistic Regression, Random Forest) and evaluate them using appropriate metrics like Precision, Recall, and the F1-Score to determine the most effective approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2c4c7d-0734-49e2-aa66-28c2d59dd282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure necessary libraries are installed, imbalanced-learn is key for SMOTE\n",
    "# !pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef9aaa5-d26e-4f9e-991c-e738c7da9e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries for data manipulation, visualization, and modeling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda-markdown-1",
   "metadata": {},
   "source": [
    "### 2. Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129d91ba-b7ae-410b-8ac6-02be286ce87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset and display the first few rows to understand its structure\n",
    "df = pd.read_csv(\"creditcard.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05dd3b0-daef-4472-8595-7eca69fb5909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a concise summary of the dataframe, checking for data types and null values\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda-markdown-2",
   "metadata": {},
   "source": [
    "### 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484ff4e3-80b3-496b-9d8b-4fb807775481",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the visual style of the plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "#Check the distribution of the 'Class' column to visualize the imbalance\n",
    "print(\"Class Distribution Counts:\")\n",
    "print(df['Class'].value_counts())\n",
    "\n",
    "#For better visualisation let's see this as a percentage\n",
    "print(\"\\nClass Distribution Percentage:\")\n",
    "print(df['Class'].value_counts(normalize=True) * 100)\n",
    "\n",
    "#Visualize it with a count plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='Class', data=df)\n",
    "plt.title(\"Class Distribution (0: Legitimate  //  1: Fraudulent)\")\n",
    "plt.ylabel(\"Number of Transations\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72de0d0f-6c58-4967-923d-ae647e483567",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analyze the 'Amount' and 'Time' features, which are the only non-anonymized columns\n",
    "print(\"Descriptive Statistics for Transaction Amount:\")\n",
    "print(df['Amount'].describe())\n",
    "\n",
    "#Comparing the 'Amount' for both classes\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "#Distribution of all transation amounts\n",
    "sns.histplot(df['Amount'], bins=50, ax=ax1, color='b', kde=True)\n",
    "ax1.set_title(\"Distrubution of All Transaction Amounts\")\n",
    "# Limiting the x-axis for better readability as amounts are heavily skewed\n",
    "ax1.set_xlim(0, 500)\n",
    "\n",
    "#Distribution of transation amounts for fraudulent transactions\n",
    "sns.histplot(df[df['Class'] == 1]['Amount'], bins=50, ax=ax2, color='r', kde=True)\n",
    "ax2.set_title(\"Distribution of Fraudulent Transaction Amounts\")\n",
    "\n",
    "plt.suptitle(\"Transaction Amount Analysis\")\n",
    "plt.show()\n",
    "\n",
    "#Analyze the 'Time' feature\n",
    "#We'll create a new plot to see how transactions are distributed over time\n",
    "# The dips suggest a cyclical pattern, likely representing nighttime hours.\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(df['Time'], bins=100, kde=True)\n",
    "plt.title(\"Distribution of Transations Over Time\")\n",
    "plt.xlabel(\"Time (in seconds)\")\n",
    "plt.ylabel(\"Number of Transactions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocessing-markdown",
   "metadata": {},
   "source": [
    "### 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a818f4e8-0965-42bb-be60-234cacc599f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the 'Time' and 'Amount' features to be on a similar scale as the PCA components (V1-V28)\n",
    "# This prevents models from being biased towards features with larger values.\n",
    "\n",
    "#Create the StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#Create copies of the columns to be scaled\n",
    "df['scaled_Amount'] = scaler.fit_transform(df[\"Amount\"].values.reshape(-1, 1))\n",
    "df['scaled_Time'] = scaler.fit_transform(df[\"Time\"].values.reshape(-1, 1))\n",
    "\n",
    "#Drop the original columns as they are now replaced by their scaled versions\n",
    "df.drop(['Time', 'Amount'], axis=1, inplace=True)\n",
    "\n",
    "print(\"Data after scaling 'Time' and 'Amount':\")\n",
    "print(df[['scaled_Amount', 'scaled_Time']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9686a7-aa80-4a30-883c-5222e99cc7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets split the data into features (X) and target (y)\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "print(\"\\nShape of X (features):\", X.shape)\n",
    "print(\"\\nShape of y (target):\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3a4cc0-c479-470e-8cbe-5f2025a4c124",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into training and testing sets\n",
    "#'stratify=y' is extremply IMPORTANT here, it ensures thats the proportion of\n",
    "#fraudulent transactionsis the same in both the training ans testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print('\\n---- Data Splitting Complete ----')\n",
    "print(\"Number of transactions in training set:\", len(X_train))\n",
    "print(\"Number of transactions in testing set:\", len(X_test))\n",
    "print(\"Distrubution of fraud in training set:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(\"Distribution of fraud in testing set:\")\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handling-imbalance-markdown",
   "metadata": {},
   "source": [
    "### 5. Handling Class Imbalance\n",
    "Our original training data (`X_train`, `y_train`) has very few fraud examples. If we train a model on it directly, the model will be lazy and biased. It will learn that it can achieve over 99% accuracy by simply guessing \"not fraud\" every single time, making it useless for actually catching fraud.\n",
    "\n",
    "To fix this, we will resample our **training data only**. We never, ever touch the test set (`X_test`, `y_test`). The test set must remain a pure, unseen representation of the real world.\n",
    "\n",
    "We will explore two popular methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3375c3c3-2e4f-458b-b7a9-95043f8097a9",
   "metadata": {},
   "source": [
    "#### Method 1 : Random Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b3f69f-3854-45b7-b6b0-8f604fcd39ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we resample lets copy the training data for later use with SMOTE\n",
    "X_train_original = X_train.copy()\n",
    "y_train_original = y_train.copy()\n",
    "\n",
    "# Step 1 : Perform Random Undersampling\n",
    "# First we'll concatenate our training data back together\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# Separate the classes\n",
    "fraud_cases = train_data[train_data['Class'] == 1]\n",
    "legit_cases = train_data[train_data['Class'] == 0]\n",
    "\n",
    "# Randomly sample the legitimate cases to match the number of fraud cases\n",
    "legit_cases_undersampled = legit_cases.sample(n = len(fraud_cases), random_state=42)\n",
    "\n",
    "# Concatenate the frand cases and the legitimate undersampled cases\n",
    "undersampled_data = pd.concat([fraud_cases, legit_cases_undersampled])\n",
    "\n",
    "# Shuffle the dataset to mix the classes\n",
    "undersampled_data = undersampled_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Separate the features (X) and target (y) again from the undersampled data\n",
    "X_train_under = undersampled_data.drop('Class', axis=1)\n",
    "y_train_under = undersampled_data['Class']\n",
    "\n",
    "# Verify the new class distribution \n",
    "print(\"New distribution after undersampling:\")\n",
    "print(y_train_under.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lr-undersampling-markdown",
   "metadata": {},
   "source": [
    "##### Training a Baseline Model (Logistic Regression) on Undersampled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0730c89-c9a2-446f-82d4-a2611fec86e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 : Train a model on the Undersampled Data\n",
    "# We'll use Logistic Regression as our first model\n",
    "lr_model = LogisticRegression(random_state=42)\n",
    "\n",
    "#Train the model ONLY on the new balanced undersampled data \n",
    "lr_model.fit(X_train_under, y_train_under)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab64d4b-bfb0-42f7-9942-bcf212a3c02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 : Evaluate the model on the unseen test set\n",
    "# We make prediction on the untouched X_test\n",
    "y_pred = lr_model.predict(X_test)\n",
    "\n",
    "print(\"\\n--- Evalution on the Test Set ---\")\n",
    "print(\"Confusion Matrix:\")\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap=\"Blues\", xticklabels=['Legit', 'Fraud'], yticklabels=['Legit', 'Fraud'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix Undersampling')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Legitimate (0)', 'Fraudulent (1)']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a3cace-6048-4c68-ac1a-b6c539a54527",
   "metadata": {},
   "source": [
    "#### Interpreting Results from Undersampling: \n",
    "The model trained on undersampled data shows a classic trade-off:\n",
    "* **High Recall (92%):** This is excellent. It means our model successfully identified 92% of all actual fraudulent transactions in the test set. It's very effective at its primary job: catching fraud.\n",
    "\n",
    "* **Low Precision (4%):** This is the major drawback. It means that when our model raises a red flag and says \"This is Fraud!\", it's only correct 4% of the time. The other 96% of the time, it's a false alarm, which would be unacceptable in a real-world scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406752f5-9ab9-4bc1-82a4-9f2e2eea33f5",
   "metadata": {},
   "source": [
    "#### Method 2 : SMOTE (Synthetic Minority Over-sampling Technique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d880b22-bb83-4097-9bbb-b05bb07a9316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For safety and clarity, we reset to the ORIGINAL training data before applying SMOTE.\n",
    "# This ensures our experiments are independent and comparable.\n",
    "X_train = X_train_original.copy()\n",
    "y_train = y_train_original.copy()\n",
    "\n",
    "# --- Step 1 : Apply SMOTE ---\n",
    "# SMOTE creates new, synthetic fraud examples to balance the dataset.\n",
    "print(\"Original training set distribution:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"\\nNew training set distribution after SMOTE:\")\n",
    "print(y_train_smote.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lr-smote-markdown",
   "metadata": {},
   "source": [
    "##### Training a Baseline Model (Logistic Regression) on SMOTE Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c6fc82-9cf8-42b9-8e92-c69d8b1a2875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2 : Training a new model on the SMOTE data ---\n",
    "# We use the same LogisticRegression model for a fair comparison of the balancing techniques\n",
    "lr_model_smote = LogisticRegression(random_state=42)\n",
    "\n",
    "# Train the model ONLY on the new, SMOTE_balanced data\n",
    "lr_model_smote.fit(X_train_smote, y_train_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d6866f-8e5a-4eec-b442-054985b06f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3 : Evaluate the SMOTE Model on the UNSEEN test set ---\n",
    "y_pred_smote = lr_model_smote.predict(X_test)\n",
    "\n",
    "print(\"\\n--- Evaluation on the Test Set (SMOTE) ---\")\n",
    "print(\"Confusion Matrix:\")\n",
    "conf_matrix_smote = confusion_matrix(y_test, y_pred_smote)\n",
    "sns.heatmap(conf_matrix_smote, annot=True, fmt='d', cmap='Greens', xticklabels=['Legit', 'Fraud'], yticklabels=['Legit', 'Fraud'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix - SMOTE')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClassification Report (SMOTE):\")\n",
    "print(classification_report(y_test, y_pred_smote, target_names=['Legitimate (0)', 'Fraudulent (1)']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7cacbb-9b49-406e-b346-17e038da08d3",
   "metadata": {},
   "source": [
    "### 6. Comparing Balancing Techniques and Choosing a Powerful Model\n",
    "#### Key Findings from Logistic Regression Experiments\n",
    "* **Excellent Recall Across Both Methods:** Both techniques successfully produced models with a high **recall of 92%**. This is a strong result, indicating that both models are capable of identifying the vast majority of actual fraudulent transactions they encounter.\n",
    "\n",
    "* **SMOTE's Superiority in Precision:** The crucial difference lies in the **precision**. The model trained on SMOTE-balanced data achieved a precision of **6%**, a 50% relative improvement over the 4% from the undersampling model.\n",
    "\n",
    "* **The Business Impact:** While a 6% precision is still low, this improvement is significant. It means the SMOTE model produces substantially fewer \"false positives.\" For every 100 transactions flagged as fraud, the SMOTE model is wrong 94 times, whereas the undersampling model is wrong 96 times. This reduction in false alarms directly translates to a better customer experience and lower operational costs from investigating incorrect flags.\n",
    "\n",
    "\n",
    "##### **Conclusion:** Based on this analysis, **SMOTE is the superior data balancing strategy** for this problem. It achieves the same excellent fraud detection rate (recall) as undersampling while creating a more precise and practical model that minimizes disruption to legitimate customers.\n",
    "\n",
    "##### **Next Step:** Having established SMOTE as our preferred data handling technique, the next logical step is to see if a more powerful algorithm, such as a **Random Forest**, can further improve upon these results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rf-markdown",
   "metadata": {},
   "source": [
    "### 7. Final Model: Random Forest with SMOTE Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e39536-f20f-40d6-ba48-eea9f3fe0866",
   "metadata": {},
   "source": [
    "# --- Step 1 : Initialize and Train the Random Forest Classifier ---\n",
    "\n",
    "print(\"Training the Random Forest model... (This may take a moment)\")\n",
    "\n",
    "# We are using the SMOTE balanced data\n",
    "# X_train_smote, y_train_smote\n",
    "\n",
    "# Initialize the model\n",
    "# n_estimators = the number of trees in the forest\n",
    "# n_jobs = -1 uses all available CPU cores to speed up training\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Stop the timer and print the duration\n",
    "end_time = time.time()\n",
    "print(f\"Random Forest training finished in {end_time - start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de0eb2b-2c3e-49bc-aaff-962ca803dc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2 : Evaluate the Random Forest Model on the Test Set ---\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "print(\"\\n--- Evaluation on the Test Set (Random Forest) ---\")\n",
    "print(\"Confusion Matrix:\")\n",
    "conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "sns.heatmap(conf_matrix_rf, annot=True, fmt='d', cmap='Oranges', xticklabels=['Legit', 'Fraud'], yticklabels=['Legit', 'Fraud'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix (Random Forest)')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClassification Report (Random Forest):\")\n",
    "print(classification_report(y_test, y_pred_rf, target_names=['Legitimate (0)', 'Fraudulent (1)']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion-markdown",
   "metadata": {},
   "source": [
    "### 8. Final Conclusion\n",
    "\n",
    "This project successfully demonstrated an end-to-end workflow for building a credit card fraud detection model on a highly imbalanced dataset.\n",
    "\n",
    "**Key Achievements:**\n",
    "* **Data Strategy is Crucial:** The initial baseline model trained on undersampled data had excellent recall (92%) but impractically low precision (4%). By switching to the **SMOTE** oversampling technique, we maintained the high recall while improving precision to 6%, significantly reducing false positives.\n",
    "* **Advanced Models Yield Superior Results:** By applying a **Random Forest** classifier to the superior SMOTE-balanced data, we achieved a massive leap in performance. Our final model produced:\n",
    "    * **Precision: 82%**\n",
    "    * **Recall: 82%**\n",
    "* **A Practical and Balanced Model:** The final model represents an excellent balance between identifying fraudulent transactions (recall) and minimizing incorrect flags on legitimate ones (precision). An F1-score of 0.82 for the fraudulent class indicates a robust and effective final model that could be confidently considered for a real-world application."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}