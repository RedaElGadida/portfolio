{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d4b76fd",
   "metadata": {},
   "source": [
    "# Loan Default Prediction\n",
    "This notebook presents a machine learning pipeline to predict loan default risk based on borrower data.\n",
    "\n",
    "**Author:** Reda El Gadida  \n",
    "**Tools:** Python, Pandas, Scikit-learn, Seaborn, Matplotlib, Jupyter Notebook\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17b4dc4",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "The goal of this project is to develop a classification model that predicts whether a loan applicant is likely to default. We simulate a real-world scenario faced by financial institutions in assessing credit risk.\n",
    "\n",
    "### Key Objectives:\n",
    "- Load and understand the data\n",
    "- Clean and preprocess the data\n",
    "- Train and evaluate a machine learning model\n",
    "- Interpret model results and performance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4b8c8a",
   "metadata": {},
   "source": [
    "![COUR_IPO.png](attachment:COUR_IPO.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9291326",
   "metadata": {},
   "source": [
    "## Understanding the Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e23be7",
   "metadata": {},
   "source": [
    "### Train vs. Test\n",
    "In this competition, you’ll gain access to two datasets that are samples of past borrowers of a financial institution that contain information about the individual and the specific loan. One dataset is titled `train.csv` and the other is titled `test.csv`.\n",
    "\n",
    "`train.csv` contains 70% of the overall sample (255,347 borrowers to be exact) and importantly, will reveal whether or not the borrower has defaulted on their loan payments (the “ground truth”).\n",
    "\n",
    "The `test.csv` dataset contains the exact same information about the remaining segment of the overall sample (109,435 borrowers to be exact), but does not disclose the “ground truth” for each borrower. It’s your job to predict this outcome!\n",
    "\n",
    "Using the patterns you find in the `train.csv` data, predict whether the borrowers in `test.csv` will default on their loan payments, or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5084ff",
   "metadata": {},
   "source": [
    "### Dataset descriptions\n",
    "Both `train.csv` and `test.csv` contain one row for each unique Loan. For each Loan, a single observation (`LoanID`) is included during which the loan was active. \n",
    "\n",
    "In addition to this identifier column, the `train.csv` dataset also contains the target label for the task, a binary column `Default` which indicates if a borrower has defaulted on payments.\n",
    "\n",
    "Besides that column, both datasets have an identical set of features that can be used to train your model to make predictions. Below you can see descriptions of each feature. Familiarize yourself with them so that you can harness them most effectively for this machine learning task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5e35bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Block\n",
    "import pandas as pd\n",
    "data_descriptions = pd.read_csv('data_descriptions.csv')\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "data_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b68dc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "\n",
    "# Data packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine Learning / Classification packages\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# Visualization Packages\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c4c310",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "Let's start by loading the dataset `train.csv` into a dataframe `train_df`, and `test.csv` into a dataframe `test_df` and display the shape of the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009dedf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Block\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "print('train_df Shape:', train_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b9dae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Block\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "print('test_df Shape:', test_df.shape)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b5a7f5",
   "metadata": {},
   "source": [
    "## Explore, Clean, Validate, and Visualize the Data (optional)\n",
    "\n",
    "Feel free to explore, clean, validate, and visualize the data however you see fit for this competition to help determine or optimize your predictive model. Please note - the final autograding will only be on the accuracy of the `prediction_df` predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613e9da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here (optional)\n",
    "X_train = train_df.drop(columns=['LoanID', 'Default'])\n",
    "y_train = train_df['Default']\n",
    "X_test = test_df.drop(columns='LoanID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c46084",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode categorical variables using one hot encoding\n",
    "categorical_cols = X_train.select_dtypes(include = \"object\").columns\n",
    "X_train_encoded = pd.get_dummies(X_train, columns=categorical_cols)\n",
    "X_test_encoded = pd.get_dummies(X_test, columns=categorical_cols)\n",
    "\n",
    "X_train_encoded, X_test_encoded = X_train_encoded.align(X_test_encoded, join = \"left\", axis=1, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3883ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Block\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_encoded)\n",
    "X_test_scaled = scaler.transform(X_test_encoded)\n",
    "\n",
    "#validation split\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_train_scaled, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882b8a4f",
   "metadata": {},
   "source": [
    "## Make predictions (required)\n",
    "\n",
    "Remember you should create a dataframe named `prediction_df` with exactly 109,435 entries plus a header row attempting to predict the likelihood of borrowers to default on their loans in `test_df`. Your submission will throw an error if you have extra columns (beyond `LoanID` and `predicted_probaility`) or extra rows.\n",
    "\n",
    "The file should have exactly 2 columns:\n",
    "`LoanID` (sorted in any order)\n",
    "`predicted_probability` (contains your numeric predicted probabilities between 0 and 1, e.g. from `estimator.predict_proba(X, y)[:, 1]`)\n",
    "\n",
    "The naming convention of the dataframe and columns are critical for our autograding, so please make sure to use the exact naming conventions of `prediction_df` with column names `LoanID` and `predicted_probability`!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a83c0f",
   "metadata": {},
   "source": [
    "### Example prediction submission:\n",
    "\n",
    "The code below is a very naive prediction method that simply predicts loan defaults using a Dummy Classifier. This is used as just an example showing the submission format required. Please change/alter/delete this code below and create your own improved prediction methods for generating `prediction_df`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c378c7fb",
   "metadata": {},
   "source": [
    "**PLEASE CHANGE CODE BELOW TO IMPLEMENT YOUR OWN PREDICTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e5659d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Block\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train_final, y_train_final)\n",
    "rf_val_proba = rf_model.predict_proba(X_val)[:, 1]\n",
    "rf_auc = roc_auc_score(y_val, rf_val_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e926654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Block\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "gb_model.fit(X_train_final, y_train_final)\n",
    "gb_val_proba = gb_model.predict_proba(X_val)[:, 1]\n",
    "gb_auc = roc_auc_score(y_val, gb_val_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea77cf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train XGBOOST\n",
    "xgb_model = XGBClassifier(use_label_encode=False, eval_metric='logloss', random_state=42)\n",
    "xgb_model.fit(X_train_final, y_train_final)\n",
    "xgb_val_proba = xgb_model.predict_proba(X_val)[:, 1]\n",
    "xgb_auc = roc_auc_score(y_val, xgb_val_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c6a010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display AUC scores\n",
    "print(f\"Random Forest AUC: {rf_auc:.4f}\")\n",
    "print(f\"Gradient Boosting AUC: {gb_auc:.4f}\")\n",
    "print(f\"XGBoost AUC: {xgb_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391445ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the final model with GradientBoostingClassifier\n",
    "final_model = GradientBoostingClassifier(random_state=42)\n",
    "final_model.fit(X_train_final, y_train_final)\n",
    "y_test_proba = final_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "prediction_df =pd.DataFrame({\n",
    "    'LoanID': test_df['LoanID'],\n",
    "    'predicted_probability': y_test_proba\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a295d3bb",
   "metadata": {},
   "source": [
    "**PLEASE CHANGE CODE ABOVE TO IMPLEMENT YOUR OWN PREDICTIONS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7d0696",
   "metadata": {},
   "source": [
    "## Final Tests - **IMPORTANT** - the cells below must be run prior to submission\n",
    "\n",
    "Below are some tests to ensure your submission is in the correct format for autograding. The autograding process accepts a csv `prediction_submission.csv` which we will generate from our `prediction_df` below. Please run the tests below an ensure no assertion errors are thrown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10129eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL TEST CELLS - please make sure all of your code is above these test cells\n",
    "\n",
    "# Writing to csv for autograding purposes\n",
    "prediction_df.to_csv(\"prediction_submission.csv\", index=False)\n",
    "submission = pd.read_csv(\"prediction_submission.csv\")\n",
    "\n",
    "assert isinstance(submission, pd.DataFrame), 'You should have a dataframe named prediction_df.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a65f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL TEST CELLS - please make sure all of your code is above these test cells\n",
    "\n",
    "assert submission.columns[0] == 'LoanID', 'The first column name should be CustomerID.'\n",
    "assert submission.columns[1] == 'predicted_probability', 'The second column name should be predicted_probability.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1721ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL TEST CELLS - please make sure all of your code is above these test cells\n",
    "\n",
    "assert submission.shape[0] == 109435, 'The dataframe prediction_df should have 109435 rows.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b0b131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL TEST CELLS - please make sure all of your code is above these test cells\n",
    "\n",
    "assert submission.shape[1] == 2, 'The dataframe prediction_df should have 2 columns.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87a4c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL TEST CELLS - please make sure all of your code is above these test cells\n",
    "\n",
    "## This cell calculates the auc score and is hidden. Submit Assignment to see AUC score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9298b949",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We successfully implemented a machine learning workflow for predicting loan defaults using logistic regression. This pipeline can be expanded to include advanced models, cross-validation, or deployed for production use. Further improvements could include feature selection, hyperparameter tuning, and model explainability.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
