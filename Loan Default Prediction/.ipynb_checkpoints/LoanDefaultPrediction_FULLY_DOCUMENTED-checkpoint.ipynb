{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "688205d3",
   "metadata": {},
   "source": [
    "# Loan Default Prediction\n",
    "This notebook demonstrates a supervised machine learning pipeline for predicting whether a loan will default based on applicant data.\n",
    "\n",
    "**Author:** Reda El Gadida  \n",
    "**Environment:** Python, Pandas, NumPy, Scikit-learn, Seaborn, Matplotlib\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1977c9",
   "metadata": {},
   "source": [
    "## Objective\n",
    "Build a binary classification model to help financial institutions identify high-risk loan applicants. This will support better decision-making in the loan approval process.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83aaf2f",
   "metadata": {},
   "source": [
    "![COUR_IPO.png](attachment:COUR_IPO.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c7307c",
   "metadata": {},
   "source": [
    "## Understanding the Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c296dc89",
   "metadata": {},
   "source": [
    "### Train vs. Test\n",
    "In this competition, you’ll gain access to two datasets that are samples of past borrowers of a financial institution that contain information about the individual and the specific loan. One dataset is titled `train.csv` and the other is titled `test.csv`.\n",
    "\n",
    "`train.csv` contains 70% of the overall sample (255,347 borrowers to be exact) and importantly, will reveal whether or not the borrower has defaulted on their loan payments (the “ground truth”).\n",
    "\n",
    "The `test.csv` dataset contains the exact same information about the remaining segment of the overall sample (109,435 borrowers to be exact), but does not disclose the “ground truth” for each borrower. It’s your job to predict this outcome!\n",
    "\n",
    "Using the patterns you find in the `train.csv` data, predict whether the borrowers in `test.csv` will default on their loan payments, or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa69adb",
   "metadata": {},
   "source": [
    "### Dataset descriptions\n",
    "Both `train.csv` and `test.csv` contain one row for each unique Loan. For each Loan, a single observation (`LoanID`) is included during which the loan was active. \n",
    "\n",
    "In addition to this identifier column, the `train.csv` dataset also contains the target label for the task, a binary column `Default` which indicates if a borrower has defaulted on payments.\n",
    "\n",
    "Besides that column, both datasets have an identical set of features that can be used to train your model to make predictions. Below you can see descriptions of each feature. Familiarize yourself with them so that you can harness them most effectively for this machine learning task!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7458f2bc",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "We begin by importing the dataset and loading it into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf8bade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_descriptions = pd.read_csv('data_descriptions.csv')\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "data_descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b5b315",
   "metadata": {},
   "source": [
    "### Split Data\n",
    "We split the dataset into training and testing subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaa6975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "\n",
    "# Data packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine Learning / Classification packages\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# Visualization Packages\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60191e9e",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "Let's start by loading the dataset `train.csv` into a dataframe `train_df`, and `test.csv` into a dataframe `test_df` and display the shape of the dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faef6583",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "We begin by importing the dataset and loading it into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d2d368",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "print('train_df Shape:', train_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f1f0a7",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "We begin by importing the dataset and loading it into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636f68f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"test.csv\")\n",
    "print('test_df Shape:', test_df.shape)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1ac41a",
   "metadata": {},
   "source": [
    "## Explore, Clean, Validate, and Visualize the Data (optional)\n",
    "\n",
    "Feel free to explore, clean, validate, and visualize the data however you see fit for this competition to help determine or optimize your predictive model. Please note - the final autograding will only be on the accuracy of the `prediction_df` predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5275f50",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "We handle missing values and drop unnecessary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc9a030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here (optional)\n",
    "X_train = train_df.drop(columns=['LoanID', 'Default'])\n",
    "y_train = train_df['Default']\n",
    "X_test = test_df.drop(columns='LoanID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6f3f2c",
   "metadata": {},
   "source": [
    "### Additional Processing\n",
    "Additional preprocessing or analysis steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac1a388",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode categorical variables using one hot encoding\n",
    "categorical_cols = X_train.select_dtypes(include = \"object\").columns\n",
    "X_train_encoded = pd.get_dummies(X_train, columns=categorical_cols)\n",
    "X_test_encoded = pd.get_dummies(X_test, columns=categorical_cols)\n",
    "\n",
    "X_train_encoded, X_test_encoded = X_train_encoded.align(X_test_encoded, join = \"left\", axis=1, fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a10ebf",
   "metadata": {},
   "source": [
    "### Split Data\n",
    "We split the dataset into training and testing subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2215bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_encoded)\n",
    "X_test_scaled = scaler.transform(X_test_encoded)\n",
    "\n",
    "#validation split\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_train_scaled, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e14cc0",
   "metadata": {},
   "source": [
    "## Make predictions (required)\n",
    "\n",
    "Remember you should create a dataframe named `prediction_df` with exactly 109,435 entries plus a header row attempting to predict the likelihood of borrowers to default on their loans in `test_df`. Your submission will throw an error if you have extra columns (beyond `LoanID` and `predicted_probaility`) or extra rows.\n",
    "\n",
    "The file should have exactly 2 columns:\n",
    "`LoanID` (sorted in any order)\n",
    "`predicted_probability` (contains your numeric predicted probabilities between 0 and 1, e.g. from `estimator.predict_proba(X, y)[:, 1]`)\n",
    "\n",
    "The naming convention of the dataframe and columns are critical for our autograding, so please make sure to use the exact naming conventions of `prediction_df` with column names `LoanID` and `predicted_probability`!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804eecca",
   "metadata": {},
   "source": [
    "### Example prediction submission:\n",
    "\n",
    "The code below is a very naive prediction method that simply predicts loan defaults using a Dummy Classifier. This is used as just an example showing the submission format required. Please change/alter/delete this code below and create your own improved prediction methods for generating `prediction_df`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb774fe6",
   "metadata": {},
   "source": [
    "**PLEASE CHANGE CODE BELOW TO IMPLEMENT YOUR OWN PREDICTIONS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697c5429",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "We train a logistic regression model on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1739a36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train_final, y_train_final)\n",
    "rf_val_proba = rf_model.predict_proba(X_val)[:, 1]\n",
    "rf_auc = roc_auc_score(y_val, rf_val_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d31f52e",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "We train a logistic regression model on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a4810f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "gb_model.fit(X_train_final, y_train_final)\n",
    "gb_val_proba = gb_model.predict_proba(X_val)[:, 1]\n",
    "gb_auc = roc_auc_score(y_val, gb_val_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd69699f",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "We train a logistic regression model on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fab813",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train XGBOOST\n",
    "xgb_model = XGBClassifier(use_label_encode=False, eval_metric='logloss', random_state=42)\n",
    "xgb_model.fit(X_train_final, y_train_final)\n",
    "xgb_val_proba = xgb_model.predict_proba(X_val)[:, 1]\n",
    "xgb_auc = roc_auc_score(y_val, xgb_val_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4d547b",
   "metadata": {},
   "source": [
    "### Additional Processing\n",
    "Additional preprocessing or analysis steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb530440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display AUC scores\n",
    "print(f\"Random Forest AUC: {rf_auc:.4f}\")\n",
    "print(f\"Gradient Boosting AUC: {gb_auc:.4f}\")\n",
    "print(f\"XGBoost AUC: {xgb_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbda3e2",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "We train a logistic regression model on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c55bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the final model with GradientBoostingClassifier\n",
    "final_model = GradientBoostingClassifier(random_state=42)\n",
    "final_model.fit(X_train_final, y_train_final)\n",
    "y_test_proba = final_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "prediction_df =pd.DataFrame({\n",
    "    'LoanID': test_df['LoanID'],\n",
    "    'predicted_probability': y_test_proba\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc22db97",
   "metadata": {},
   "source": [
    "**PLEASE CHANGE CODE ABOVE TO IMPLEMENT YOUR OWN PREDICTIONS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18491538",
   "metadata": {},
   "source": [
    "## Final Tests - **IMPORTANT** - the cells below must be run prior to submission\n",
    "\n",
    "Below are some tests to ensure your submission is in the correct format for autograding. The autograding process accepts a csv `prediction_submission.csv` which we will generate from our `prediction_df` below. Please run the tests below an ensure no assertion errors are thrown."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edf4eba",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "We begin by importing the dataset and loading it into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e750f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL TEST CELLS - please make sure all of your code is above these test cells\n",
    "\n",
    "# Writing to csv for autograding purposes\n",
    "prediction_df.to_csv(\"prediction_submission.csv\", index=False)\n",
    "submission = pd.read_csv(\"prediction_submission.csv\")\n",
    "\n",
    "assert isinstance(submission, pd.DataFrame), 'You should have a dataframe named prediction_df.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89703779",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "We use accuracy metrics to evaluate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077fe8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL TEST CELLS - please make sure all of your code is above these test cells\n",
    "\n",
    "assert submission.columns[0] == 'LoanID', 'The first column name should be CustomerID.'\n",
    "assert submission.columns[1] == 'predicted_probability', 'The second column name should be predicted_probability.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370079c0",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "We use accuracy metrics to evaluate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db067f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL TEST CELLS - please make sure all of your code is above these test cells\n",
    "\n",
    "assert submission.shape[0] == 109435, 'The dataframe prediction_df should have 109435 rows.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca30d659",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "We use accuracy metrics to evaluate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6259985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL TEST CELLS - please make sure all of your code is above these test cells\n",
    "\n",
    "assert submission.shape[1] == 2, 'The dataframe prediction_df should have 2 columns.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203dbeea",
   "metadata": {},
   "source": [
    "### Additional Processing\n",
    "Additional preprocessing or analysis steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa50f5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL TEST CELLS - please make sure all of your code is above these test cells\n",
    "\n",
    "## This cell calculates the auc score and is hidden. Submit Assignment to see AUC score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b87225d",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This notebook showcased a complete ML pipeline using logistic regression to predict loan default risk. Future improvements could include testing additional algorithms, feature engineering, or model explainability tools.\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
